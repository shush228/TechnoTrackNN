{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kaggle_ver_4.ipynb","provenance":[{"file_id":"1E59qG8fcZicSo-hguHhT7c5Bpmst0lCt","timestamp":1620065590762}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7PkeTIjziYN5"},"source":["import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torch.autograd import Variable\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from sklearn.utils import shuffle\n","from tqdm import tqdm\n","from typing import Tuple, List, Type, Dict, Any"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQ5lRJoamann"},"source":["import cv2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hG0c8in0G3QS"},"source":["from queue import Empty, Queue\n","from threading import Thread\n","import threading\n","#Библиотеки для потоков"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCCljdZy6xkn"},"source":["# augmentation library\n","from imgaug.augmentables import Keypoint, KeypointsOnImage\n","import imgaug.augmenters as iaa \n","#import accimage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-v7l1yjrmHk"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1aEuREpkPwc"},"source":["with open('/content/drive/MyDrive/geo_kaggle/index.pkl', 'rb') as f:\n","    data_index = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBpVON_WQ_-e"},"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    print('Using GPU', f'({torch.cuda.get_device_name(), torch.cuda.get_device_properties(device)})')\n","else:\n","    device = torch.device('cpu')\n","    print('Using CPU')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKgbgU-MrmRQ"},"source":["class thread_killer(object):    \n","    \"\"\"Boolean object for signaling a worker thread to terminate\n","    Once a thread is launched, it should be terminated at some moment.\n","    In case the function of this thread is an infinite loop, one needs a mutex\n","    for signaling a worker thread to break the loop.\n","    The fuction will return, and the thread will be terminated.\n","    \"\"\"\n","    \n","    def __init__(self):\n","        self.to_kill = False\n","\n","    def __call__(self):\n","        return self.to_kill\n","\n","    def set_tokill(self, tokill):\n","        self.to_kill = tokill"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZFz8J4VIYa9"},"source":["def threaded_batches_feeder(tokill, batches_queue, dataset_generator):\n","    \"\"\"\n","    Threaded worker for pre-processing input data.\n","    tokill (thread_killer): an object that indicates whether a thread should be terminated\n","    dataset_generator (Dataset): training/validation data generator\n","    batches_queue (Queue): a limited size thread-safe Queue instance for train/validation data batches\n","    \"\"\"\n","    while tokill() == False:\n","        for sample_batch in dataset_generator:\n","            \n","            batches_queue.put(sample_batch, block=True)\n","            \n","            if tokill() == True:\n","                return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZAcbajoIsFC"},"source":["def threaded_cuda_batches(tokill, cuda_batches_queue, batches_queue):\n","    \"\"\"\n","    Thread worker for transferring pytorch tensors into GPU. \n","    batches_queue (Queue): the queue that fetches numpy cpu tensors.\n","    cuda_batches_queue (Queue): the queue receiving numpy cpu tensors and transfering them to GPU memory.\n","    \"\"\"\n","    while tokill() == False:\n","        sample_batch,labels,ids = batches_queue.get(block=True)\n","        sample_batch = Variable(sample_batch).to(device)\n","        labels = labels.to(device)\n","        ids = ids.to(device)\n","        \n","        cuda_batches_queue.put((sample_batch,labels,ids), block=True)\n","        if tokill() == True:\n","            return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpYDNk41NSug"},"source":["class Threadsafe_iter:\n","    \"\"\"\n","    Takes an iterator/generator and makes it thread-safe by\n","    serializing call to the `next` method of given iterator/generator.\n","    \"\"\"\n","    def __init__(self, it):\n","        self.it = it\n","        self.lock = threading.Lock()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        with self.lock:\n","            return next(self.it)\n","\n","def get_objects_id(objects_count):\n","    \"\"\"Cyclic generator of paths indices\"\"\"\n","    current_objects_id = 0\n","    while True:\n","        yield current_objects_id\n","        current_objects_id  = (current_objects_id + 1) % objects_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMFoqCtSjmYL"},"source":["class SkyDataset(Dataset):    \n","    def __init__(self, \n","                 data_frame, \n","                 root_dir, \n","                 transform=None, \n","                 batch_size = 8, \n","                 augment = True,\n","                 seq = iaa.Sequential([iaa.GaussianBlur(sigma=(0, 5))],random_order=True), \n","                 train = True, \n","                 target = True):\n","        \"\"\"\n","        Args:\n","            pkl_file (string): Path to the pkl file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","            batch_size (int, optional): batch size\n","        \"\"\"\n","        self.is_train = train\n","        self.with_target = target\n","        self.sky_data = data_frame\n","        self.root_dir = os.path.abspath(root_dir)\n","        self.transform = transform\n","        \n","        self.batch_size = batch_size\n","        \n","        self.objects_id_generator = Threadsafe_iter(get_objects_id(self.sky_data.shape[0]))\n","        \n","        self.lock = threading.Lock()\n","        self.yield_lock = threading.Lock()\n","        self.init_count = 0\n","        self.augment = augment\n","        self.cache = {}\n","        \n","        if self.augment:\n","            # instantiate augmentations\n","            self.seq = seq\n","        \n","\n","       \n","\n","\n","    def __len__(self):                        \n","        return self.sky_data.shape[0]\n","    \n","\n","    def shuffle(self):\n","        self.sky_data = shuffle(self.sky_data).reset_index(drop=True)\n","    \n","    def __iter__(self):\n","        while True:\n","            with self.lock:\n","                if (self.init_count == 0):\n","                    if self.is_train:\n","                        self.shuffle()\n","                    self.imgs = []\n","                    self.labels = []\n","                    self.ids =[]\n","                    self.init_count = 1\n","            \n","            \n","            for obj_id in self.objects_id_generator:\n","                add_str =''\n","                if(self.with_target and self.sky_data.iloc[obj_id]['mission'] == 'AI49'):\n","                    add_str = 'ai49-'\n","                \n","                \n","                \n","                img_name = os.path.join(self.root_dir, self.sky_data.iloc[obj_id]['mission'], 'snapshots', add_str + 'snapshots-'+str((self.sky_data.iloc[obj_id]['observations_dt']).date()), self.sky_data.iloc[obj_id]['jpg_filename'])\n","                mask_name = os.path.join(self.root_dir, self.sky_data.iloc[obj_id]['mission'], 'masks', 'mask-id'+str(self.sky_data.iloc[obj_id]['devID'])+'.png') \n","                if (self.with_target):\n","                  label = int(self.sky_data.iloc[obj_id]['observed_TCC'])      \n","                \n","                  \n","               \n","                #Очень затратная шляпа по времени\n","               # mask = plt.imread(mask_name)\n","               # mask = np.where(mask == 255, np.ones_like(mask),mask*0)\n","                       \n","                \n","                \n","                img = plt.imread(img_name)\n","                img = torchvision.transforms.Compose([\n","                                                      torchvision.transforms.ToPILImage(), \n","                                                     torchvision.transforms.ToTensor(),\n","                                                    torchvision.transforms.Resize([256, 256])\n","                                                    ])(img)\n","                if self.transform:\n","                    img = self.transform(img)\n","                      \n","\n","                \n","                img= img.numpy()\n","                \n","\n","                                  \n","                if self.augment:\n","                    img = self.seq(images = img)\n","                \n","                \n","                \n","                    # Concurrent access by multiple threads to the lists below\n","                with self.yield_lock:\n","                    if (len(self.imgs)) < self.batch_size:\n","                        self.imgs.append(img)\n","                        if self.with_target:\n","                            self.labels.append(label)\n","                        if self.with_target == False :\n","                            self.ids.append(obj_id)\n","                    if (obj_id + 1 == len(self)):\n","                        yield (torch.Tensor(np.array(self.imgs)),(torch.Tensor(self.labels)).type(torch.LongTensor), torch.Tensor(self.ids))\n","                        self.imgs = []\n","                        self.labels = []\n","                        self.jpg_names =[]\n","                        break    \n","                    if len(self.imgs) % self.batch_size == 0:\n","                     \n","                        yield (torch.Tensor(np.array(self.imgs)),(torch.Tensor(self.labels)).type(torch.LongTensor), torch.Tensor(self.ids))\n","                        self.imgs = []\n","                        self.labels = []\n","                        self.ids =[]\n","\n","                    \n","            # At the end of an epoch we re-init data-structures\n","            with self.lock:\n","                if self.is_train:\n","                    self.sky_data = shuffle(self.sky_data)\n","                self.init_count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHBt9jZh0PnG"},"source":["def train_single_epoch(model: torch.nn.Module,\n","                       optimizer: torch.optim.Optimizer, \n","                       loss_function: torch.nn.Module, \n","                       STEPS_PER_EPOCH,\n","                       train_cuda_batches_queue,\n","                       data_len):\n","    \n","    model.train()\n","    loss_sum = 0\n","\n","    for image_batch in tqdm(range(STEPS_PER_EPOCH), total=STEPS_PER_EPOCH):\n","      \n","        x,y, ids = train_cuda_batches_queue.get(block = True)\n","        \n","        model.zero_grad()\n","        hyp = model(x)\n","       \n","      \n","        loss = loss_function(hyp, y)\n","        loss.backward()\n","        loss_sum += loss\n","        \n","        optimizer.step()\n","\n","    \n","    return loss_sum/float(data_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Iig5xoX0az7"},"source":["@torch.no_grad()\n","def validate_single_epoch(model: torch.nn.Module,\n","                          loss_function: torch.nn.Module,                          \n","                          STEPS_PER_EPOCH,\n","                          test_cuda_batches_queue,\n","                          data_len):\n","    model.eval()\n","    loss_sum = 0\n","    accuracy = 0\n","    \n","    for image_batch in range(STEPS_PER_EPOCH):\n","        \n","        x,y,ids = test_cuda_batches_queue.get(block = True)\n","\n","        hyp = model(x)\n","        loss = loss_function(hyp, y)\n","        loss_sum += loss\n","\n","        y_pred = hyp.argmax(dim = 1, keepdim = True).to(device)\n","    \n","        accuracy += y_pred.eq(y.view_as(y_pred)).sum().item()\n","\n","    loss_avr = loss_sum / float(data_len)\n","    accuracy_avr = 100 * accuracy / float(data_len)\n","    \n","    return {'loss' : loss_avr.item(), 'accuracy' : accuracy_avr}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOuVLUYg0dM_"},"source":["def ploting_curves(loss, best_epoch):\n","    \"\"\"\n","    Plot loss evolution on training and validation sets\n","    \"\"\"\n","    # Plot learning loss curve\n","    plt.plot(loss['train'], label = 'Training set')\n","    plt.plot(loss['valid'], label = 'Val set')\n","    plt.axvline(best_epoch, color = 'r', ls = '--', label = 'Best model')\n","    plt.title('Loss evolution')\n","    plt.xlabel('epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YEEiiK9j0eq1"},"source":["import math\n","def train_model(model: torch.nn.Module, \n","                train_data,\n","                test_data,\n","                loss_function: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n","                optimizer_class: Type[torch.optim.Optimizer] = torch.optim,\n","                optimizer_params: Dict = {},\n","                initial_lr = 0.01,\n","                lr_scheduler_class: Any = torch.optim.lr_scheduler.ReduceLROnPlateau,\n","                lr_scheduler_params: Dict = {},\n","                batch_size = 16,\n","                s_eq = iaa.Sequential([iaa.GaussianBlur(sigma=(0, 5))],random_order=True),\n","                max_epochs = 1000,\n","                early_stopping_patience = 20):\n","    # set to training mode\n","  \n","    # Here we instantiate queues and mutexes, and launch the threads that will preprocess the data and send it into GPU\n","\n","    \n","    SkyData_train = SkyDataset(train_data, root_dir = '/content/drive/MyDrive/geo_kaggle', batch_size= batch_size, seq = s_eq)\n","    SkyData_test = SkyDataset(test_data, root_dir = '/content/drive/MyDrive/geo_kaggle', batch_size= batch_size, augment= False, train = False)\n","    \n","    STEPS_PER_EPOCH_TRAIN = math.ceil(len(SkyData_train)/ float(batch_size)) \n","    STEPS_PER_EPOCH_TEST = math.ceil(len(SkyData_test)/ float(batch_size)) \n","\n","    #Настроики на трайн\n","    train_batches_queue_length = min(STEPS_PER_EPOCH_TRAIN, 3)    \n","    train_batches_queue = Queue(maxsize=train_batches_queue_length)\n","    train_cuda_batches_queue = Queue(maxsize=4)\n","    train_thread_killer = thread_killer()\n","    train_thread_killer.set_tokill(False)\n","    train_preprocess_workers = 24\n","\n","    for _ in range(train_preprocess_workers):\n","        thr = Thread(target=threaded_batches_feeder, args=(train_thread_killer, train_batches_queue, SkyData_train))\n","        thr.start()\n","\n","    train_cuda_transfers_thread_killer = thread_killer()\n","    train_cuda_transfers_thread_killer.set_tokill(False)\n","    train_cudathread = Thread(target=threaded_cuda_batches, args=(train_cuda_transfers_thread_killer, train_cuda_batches_queue, train_batches_queue))\n","    train_cudathread.start()\n","\n","    #Настроики на тест  \n","    test_batches_queue_length = min(STEPS_PER_EPOCH_TEST, 3)    \n","    test_batches_queue = Queue(maxsize=test_batches_queue_length)\n","    test_cuda_batches_queue = Queue(maxsize=4)\n","    test_thread_killer = thread_killer()\n","    test_thread_killer.set_tokill(False)\n","    test_preprocess_workers = 8\n","\n","    for _ in range(test_preprocess_workers):\n","        thr = Thread(target=threaded_batches_feeder, args=(test_thread_killer, test_batches_queue, SkyData_test))\n","        thr.start()\n","\n","    test_cuda_transfers_thread_killer = thread_killer()\n","    test_cuda_transfers_thread_killer.set_tokill(False)\n","    test_cudathread = Thread(target=threaded_cuda_batches, args=(test_cuda_transfers_thread_killer, test_cuda_batches_queue, test_batches_queue))\n","    test_cudathread.start()\n","\n","    # Everything is ready for the training\n","  \n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, **optimizer_params)\n","    lr_scheduler = lr_scheduler_class(optimizer, **lr_scheduler_params)\n","    \n","    best_val_loss = None\n","    best_epoch = None\n","    loss_list = {'train': list(), 'valid': list()}\n","    \n","\n","    for epoch in range(max_epochs):\n","        \n","        print(f'Epoch {epoch}')\n","    \n","        train_loss =  train_single_epoch(model, optimizer, loss_function,STEPS_PER_EPOCH_TRAIN, train_cuda_batches_queue, len(SkyData_train))\n","        \n","        print('Validating epoch\\n')\n","\n","        val_metrics = validate_single_epoch(model, loss_function, STEPS_PER_EPOCH_TEST, test_cuda_batches_queue, len(SkyData_test))\n","        loss_list['valid'].append(val_metrics['loss'])\n","        print(f'Validation metrics: \\n{val_metrics}')\n","\n","        lr_scheduler.step(val_metrics['loss'])\n","        \n","        if best_val_loss is None or best_val_loss > val_metrics['loss']:\n","            print(f'Best model yet, saving')\n","            best_val_loss = val_metrics['loss']\n","            best_epoch = epoch\n","            torch.save(model, './best_model.pth')\n","            torch.save(model, '/content/drive/MyDrive/geo_kaggle/best_model_disk.pth')\n","            \n","        if epoch - best_epoch > early_stopping_patience:\n","            print('Early stopping triggered')\n","            ploting_curves(loss_list,best_epoch)\n","            break\n","    \n","\n","    train_thread_killer.set_tokill(True)\n","    train_cuda_transfers_thread_killer.set_tokill(True)\n","    for _ in range(train_preprocess_workers):\n","        try:\n","            # Enforcing thread shutdown\n","            train_batches_queue.get(block=True, timeout=1)\n","            train_cuda_batches_queue.get(block=True, timeout=1)\n","        except Empty:\n","            pass\n","\n","    test_thread_killer.set_tokill(True)\n","    test_cuda_transfers_thread_killer.set_tokill(True)\n","    for _ in range(test_preprocess_workers):\n","        try:\n","            # Enforcing thread shutdown\n","            test_batches_queue.get(block=True, timeout=1)\n","            test_cuda_batches_queue.get(block=True, timeout=1)\n","        except Empty:\n","            pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUxOzgyJiR2x"},"source":["\n","def GetTarget(data, model, batch = 100):\n","    SkyData_test = SkyDataset(data, root_dir = '/content/drive/MyDrive/geo_kaggle_test', batch_size= batch, augment= False, train = False, target = False)\n","    df =  pd.DataFrame(columns=['jpg_filename','TCC'])\n","    STEPS_PER_EPOCH_TEST = math.ceil(len(SkyData_test)/ float(batch)) \n","\n","    size = 0\n","    #Настроики на тест  \n","    test_batches_queue_length = min(STEPS_PER_EPOCH_TEST, 4)    \n","    test_batches_queue = Queue(maxsize=test_batches_queue_length)\n","    test_cuda_batches_queue = Queue(maxsize=4)\n","    test_thread_killer = thread_killer()\n","    test_thread_killer.set_tokill(False)\n","    test_preprocess_workers = 32\n","\n","    for _ in range(test_preprocess_workers):\n","        thr = Thread(target=threaded_batches_feeder, args=(test_thread_killer, test_batches_queue, SkyData_test))\n","        thr.start()\n","\n","    test_cuda_transfers_thread_killer = thread_killer()\n","    test_cuda_transfers_thread_killer.set_tokill(False)\n","    test_cudathread = Thread(target=threaded_cuda_batches, args=(test_cuda_transfers_thread_killer, test_cuda_batches_queue, test_batches_queue))\n","    test_cudathread.start()\n","\n","    \n","    model.eval()\n","   \n","    for image_batch in tqdm(range(STEPS_PER_EPOCH_TEST), total=STEPS_PER_EPOCH_TEST):\n","        \n","        x,y,ids = test_cuda_batches_queue.get(block = True)\n","\n","        hyp = model(x)\n","  \n","        y_pred = hyp.argmax(dim = 1, keepdim = True).to(device)\n","\n","        ids = (ids).tolist()\n","        y_pred = (y_pred).tolist()\n","        for i in range(len(ids)):\n","            df.loc[size] = [data.iloc[int(ids[i])]['jpg_filename'], y_pred[i][0]]\n","            size += 1\n","        \n","    test_thread_killer.set_tokill(True)\n","    test_cuda_transfers_thread_killer.set_tokill(True)\n","    for _ in range(test_preprocess_workers):\n","        try:\n","            # Enforcing thread shutdown\n","            test_batches_queue.get(block=True, timeout=1)\n","            test_cuda_batches_queue.get(block=True, timeout=1)\n","        except Empty:\n","            pass\n","\n","    return df.drop_duplicates()\n","         \n","    \n","        \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DddQQM2PtGec"},"source":["class Pe(torch.nn.Module):\n","    \n","    def __init__(self, \n","                 input_resolution: Tuple[int, int] = (512, 512),\n","                 input_channels: int = 1, \n","                 hidden_layer_features: List[int] = [256, 256, 256],\n","                 activation: Type[torch.nn.Module] = torch.nn.ReLU,\n","                 num_classes: int = 9):\n","        \n","        super().__init__()\n","\n","        self.input_resolution = input_resolution\n","        self.input_channels = input_channels\n","        self.hidden_layer_features = hidden_layer_features\n","        self.activation = activation\n","        self.num_classes = num_classes\n","        \n","        self.conv1 = torch.nn.Conv2d(3, 3, 32)\n","        self.conv2 = torch.nn.Conv2d(3, 3, 32, 4)\n","        self.conv3 = torch.nn.Conv2d(3, 3, 7, 7)\n","        \n","        self.fc1 = torch.nn.Linear(7*7*3, 9)\n","        \n","    def forward(self, x):\n","        \n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        \n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        \n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        \n","        x = x.view(-1, 7*7*3)\n","        x = self.fc1(x)\n","        \n","        output = F.log_softmax(x, dim = 1)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rxo7fA4I0nQn"},"source":["model = Pe()\n","model.to(device)\n","print(model)\n","print('Total number of trainable parameters', \n","      sum(p.numel() for p in model.parameters() if p.requires_grad))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Q_JKIM9YhtN"},"source":["#Если делать умную выборку, то здесь в pd\n","data = pd.DataFrame(data_index)\n","data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YR0rtcbSb6h2"},"source":["df= data[data['observed_TCC'] == 8 ].sample(10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laVVab3pc3_1"},"source":["data = pd.concat([df, data[data['observed_TCC'] != 8 ]], ignore_index=True).reset_index(drop = True)\n","data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1-nNnMQdUTR"},"source":["plt.figure(figsize = (10, 10))\n","sns.histplot(data, x = 'observed_TCC', hue = 'mission', multiple = 'stack')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFMVoBqG6k7J"},"source":["data = shuffle(data).reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9N7FU2yfWDl"},"source":["test_data = data.loc[60000:67999,:].reset_index(drop=True)\n","train_data = data.loc[:59999,:].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eumApmwk0uSz"},"source":["train_model(model, \n","            train_data,\n","            test_data,\n","            loss_function=torch.nn.CrossEntropyLoss(), \n","            initial_lr=0.0001,\n","            batch_size= 100)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw5o56jVvbZQ"},"source":["#the_model = torch.load('/content/drive/MyDrive/geo_kaggle/best_model_1.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asiAgJH5tPGC"},"source":["#df = GetTarget(data,the_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEDoTtcVLgX3"},"source":["#df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyjHIFyo6Q_i"},"source":["#df.to_csv('second_try.csv', sep=',' , index=False, header = None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVmHfrK1Je6H"},"source":["Из-за параллельных вычислений некоторые объекты не попадают в модель, вот их имена "]},{"cell_type":"code","metadata":{"id":"ZdWbyMD2FR6-"},"source":["#pd.concat([d, f]).drop_duplicates(keep=False)"],"execution_count":null,"outputs":[]}]}